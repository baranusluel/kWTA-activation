{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kWTA Activation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aoz5yR4qOGl",
        "colab_type": "text"
      },
      "source": [
        "# Resisting Adversarial Attacks by kWTA Activation\n",
        "*ICLR Reproducibility Challenge*\n",
        "\n",
        "**CS4803/7643 Spring 2020 Final Project**\n",
        "\n",
        "By: Baran Usluel and Ilya Golod"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFoCO20m9l2U",
        "colab_type": "text"
      },
      "source": [
        "## Our Plan\n",
        "\n",
        "*Will delete this cell later. For now, linking some resources and our guiding plan (copied from proposal spreadsheet):*\n",
        "\n",
        "**Paper:** https://arxiv.org/abs/1905.10510\n",
        "\n",
        "**ICLR Submission Review:** https://openreview.net/forum?id=Skgvy64tvr\n",
        "\n",
        "**Paper's Github:** https://github.com/a554b554/kWTA-Activation\n",
        "\n",
        "**Project Summary:**\n",
        "\n",
        "Implement k-WTA activation function as described in Enhancing Adversarial Defense by k-Winners-Take-All @ ICLR 2020. Reproduce empirical results (test accuracy and adversarial robustness) across the different network architectures, training methods and adversarial attacks shown in the paper. Possibly test with unexplored environments as well.\n",
        "\n",
        "**Follow-up:**\n",
        "\n",
        "The k-WTA activation function will be implemented in PyTorch on pretrained models. The architecture models tested will include those in the paper (ResNet, DenseNet and Wide ResNet) and possibly additional relevant models (SqueezeNet, AlexNet, VGG and so on).\n",
        "\n",
        "The white-box attack model will be examined since this is the main focus of the original paper. Specifically the attacks to be considered are: vanilla gradient ascent (as already implemented in PS2), projected gradient descent, Deepfool, Carlini-Wagner, Momentum Iterative Method, and possibly other state-of-the-art gradient-based adversarial attacks by using the Foolbox library.\n",
        "\n",
        "Since we are using pretrained models, we will only consider attacks on regularly trained models and not explore adversarial training. Note that the authors claim similar improvements with k-WTA across various training methods.\n",
        "\n",
        "Finally, we will be using the CIFAR10 dataset for the image classification task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b45jcGoHOxKf",
        "colab_type": "text"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUcHF0TpO7KH",
        "colab_type": "code",
        "outputId": "1d3b38b1-2c65-4ef3-9ac6-4535f45082e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# If using colab:\n",
        "# Mounts google drive folder so we can save/load files.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUVVE53sPOyW",
        "colab_type": "code",
        "outputId": "df7c2f70-e9ae-45c5-eb26-1098574795ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Note to team members:\n",
        "# If you get a 'No such file' error when you run this, go to your Google Drive,\n",
        "# find the CS4803 Project folder under Shared With Me, right-click and select\n",
        "# Add Shortcut To Drive. This will make the path accessible.\n",
        "DATA_DIRECTORY = \"gdrive/My Drive/CS4803 Project/\"\n",
        "import os\n",
        "print(os.listdir(DATA_DIRECTORY))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['models', 'data', 'kWTA Activation.ipynb']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FI558mq5q9mV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as T\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import copy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-7I72lWnXnc",
        "colab_type": "code",
        "outputId": "9ff929f8-07d5-439e-f36f-44c8ab280983",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# For massive speed-up, ensure GPU is selected from Runtime -> Change runtime type.\n",
        "# Using hardware acceleration:\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
        "print(device)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WTGTzUf2QJx",
        "colab_type": "text"
      },
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5YtG509gPef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0Bz5fdDt1kq",
        "colab_type": "text"
      },
      "source": [
        "## k-WTA Activation Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0cTjMyvt57_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class kWTA(nn.Module):\n",
        "    def __init__(self, sr):\n",
        "        super(kWTA, self).__init__()\n",
        "        self.sr = sr\n",
        "\n",
        "    # Modified version of paper's forward implementation\n",
        "    def forward(self, x):\n",
        "        # Custom code to work with any array size:\n",
        "        tmpx = x.view(x.shape[0], -1)\n",
        "        size = tmpx.shape[1]\n",
        "        k = int(self.sr * size)\n",
        "        # Directly taken from paper's implementation:\n",
        "        topval = tmpx.topk(k, dim=1)[0][:,-1]\n",
        "        topval = topval.repeat(tmpx.shape[1], 1).permute(1,0).view_as(x)\n",
        "        comp = (x>=topval).to(x)\n",
        "        return comp*x\n",
        "\n",
        "    # TODO: Is there a more efficient way of computing this?\n",
        "\n",
        "    # # An alternate implementation:\n",
        "    # def forward(self, x):\n",
        "    #     tmpx = x.view(x.shape[0], -1)\n",
        "    #     size = tmpx.shape[1]\n",
        "    #     k = int(self.sr * size)\n",
        "    #     top_inds = tmpx.topk(k, dim=1)[1]\n",
        "    #     mask = torch.zeros_like(tmpx, dtype=torch.bool)\n",
        "    #     mask.scatter_(1, top_inds, True)\n",
        "    #     tmpx[~mask] = 0\n",
        "    #     return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2NHO7Bdz2eo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Checking to make sure kWTA forward-pass implementation is correct\n",
        "kwta = kWTA(0.2)\n",
        "a = torch.rand(2,5,5)\n",
        "print(a)\n",
        "b = kwta.forward(a)\n",
        "print(b)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wa_E4Brw8-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Replaces given activation with specified kWTA activation function\n",
        "def activation_to_kwta(model, old_activation, sr=0.2):\n",
        "    for child_name, child in model.named_children():\n",
        "        if isinstance(child, old_activation):\n",
        "            setattr(model, child_name, kWTA(sr))\n",
        "        else:\n",
        "            activation_to_kwta(child, old_activation, sr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBEU0ns62FqL",
        "colab_type": "text"
      },
      "source": [
        "## Load Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpSa_SC_G499",
        "colab_type": "code",
        "outputId": "aafcc21b-d890-4f74-ab8f-3e2c76110445",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# NOTE on normalization values:\n",
        "# Paper uses mean=0 var=1, so that's what we used here (for now).\n",
        "# But pytorch docs suggest mean=var=0.5, see https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
        "# And some sources claim other specific values:\n",
        "#   https://github.com/kuangliu/pytorch-cifar/issues/19\n",
        "#   https://github.com/kuangliu/pytorch-cifar/blob/master/main.py\n",
        "MEAN = 0\n",
        "VAR = 1\n",
        "INPUT_SIZE = 224\n",
        "# Same transforms as paper, except also resizing to 224x224 because that\n",
        "# is what torchvision models expect\n",
        "transform_train = T.Compose(\n",
        "    [T.RandomCrop(32, padding=4),\n",
        "     T.RandomHorizontalFlip(),\n",
        "     T.Resize(INPUT_SIZE),\n",
        "     T.ToTensor(),\n",
        "     T.Normalize((MEAN,MEAN,MEAN), (VAR,VAR,VAR))])\n",
        "transform_test = T.Compose(\n",
        "    [T.Resize(INPUT_SIZE),\n",
        "     T.ToTensor(),\n",
        "     T.Normalize((MEAN,MEAN,MEAN), (VAR,VAR,VAR))])\n",
        "\n",
        "trainset = datasets.CIFAR10(root=DATA_DIRECTORY+'data', train=True, download=True, transform=transform_train)\n",
        "trainloader = DataLoader(trainset, batch_size=16, shuffle=True, num_workers=2)\n",
        "\n",
        "testset = datasets.CIFAR10(root=DATA_DIRECTORY+'data', train=False, download=True, transform=transform_test)\n",
        "testloader = DataLoader(testset, batch_size=16, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88o7a1bXyRqk",
        "colab_type": "text"
      },
      "source": [
        "## Load Saved Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GGcKxcXqtb_",
        "colab_type": "text"
      },
      "source": [
        "### Pretrained Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6VDjGntyQ8r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Change pretrained model download directory, so it doesn't\n",
        "# download every time the runtime restarts\n",
        "os.environ['TORCH_HOME'] = DATA_DIRECTORY + 'models/pretrained'\n",
        "\n",
        "# We have four types of models stored here:\n",
        "# - pretrained: Default models from pytorch, trained on ImageNet\n",
        "# - relu: Fine-tuned models for CIFAR10\n",
        "# - kwta_0_1: Models using kwta activation with sparsity=0.1 for CIFAR10\n",
        "# - kwta_0_2: Models using kwta activation with sparsity=0.2 for CIFAR10\n",
        "models = {'pretrained': {}, 'relu': {}, 'kwta_0_1': {}, 'kwta_0_2': {}}\n",
        "\n",
        "# Download and load pretrained models (trained for ImageNet dataset)\n",
        "models['pretrained']['resnet'] = torchvision.models.resnet18(pretrained=True)\n",
        "models['pretrained']['densenet'] = torchvision.models.densenet121(pretrained=True)\n",
        "models['pretrained']['wide_resnet'] = torchvision.models.wide_resnet50_2(pretrained=True)\n",
        "models['pretrained']['vgg'] = torchvision.models.vgg11(pretrained=True)\n",
        "models['pretrained']['alexnet'] = torchvision.models.alexnet(pretrained=True)\n",
        "models['pretrained']['squeezenet'] = torchvision.models.squeezenet1_1(pretrained=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WqnljgSnqv0B",
        "colab_type": "text"
      },
      "source": [
        "### Our Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u9b6SPMrq2g8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "############################################################\n",
        "# These are the models that we have trained and saved.     #\n",
        "# Keep this list updated, along with EXPERIMENTAL RESULTS: #\n",
        "############################################################\n",
        "\n",
        "# Reference on finetuning models: https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html\n",
        "\n",
        "# TODO: Models could probably use at least another epoch of training.\n",
        "\n",
        "## AlexNet ReLU\n",
        "# Based off pretrained AlexNet.\n",
        "# Trained for 2 epochs, test accuracy: 84.5%\n",
        "# Vanilla attack with 10 minibatches:\n",
        "#   Attacks Succeeded: 1426 / 1440 = 99.02777777777777 %\n",
        "#   Robustness Accuracy:  4 / 1440 = 0.2777777777777778 %\n",
        "models['relu']['alexnet'] = copy.deepcopy(models['pretrained']['alexnet'])\n",
        "models['relu']['alexnet'].classifier[-1].out_features = 10\n",
        "models['relu']['alexnet'].load_state_dict(torch.load(\n",
        "        DATA_DIRECTORY + 'models/relu/AlexNet.pth'))\n",
        "\n",
        "## AlexNet kWTA 0.2\n",
        "# Based off ReLU AlexNet trained for 2 epochs.\n",
        "# Trained for 1 epoch, test accuracy: 85.1%\n",
        "# Vanilla attack with 10 minibatches:\n",
        "#   Attacks Succeeded: 1417 / 1440 = 98.40277777777777 %\n",
        "#   Robustness Accuracy:  13 / 1440 = 0.9027777777777778 %\n",
        "models['kwta_0_2']['alexnet'] = copy.deepcopy(models['relu']['alexnet'])\n",
        "activation_to_kwta(models['kwta_0_2']['alexnet'], nn.ReLU, sr=0.2)\n",
        "models['kwta_0_2']['alexnet'].load_state_dict(torch.load(\n",
        "        DATA_DIRECTORY + 'models/kwta_0_2/AlexNet.pth'))\n",
        "\n",
        "## AlexNet kWTA 0.1\n",
        "# Based off kWTA 0.2 AlexNet trained for 1 epoch.\n",
        "# Trained for 1 epoch, test accuracy: 87.0%\n",
        "# Vanilla attack with 10 minibatches:\n",
        "#   Attacks Succeeded: 696 / 1440 = 48.333333333333336 %\n",
        "#   Robustness Accuracy:  231 / 1440 = 16.041666666666668 %\n",
        "models['kwta_0_1']['alexnet'] = copy.deepcopy(models['kwta_0_2']['alexnet'])\n",
        "activation_to_kwta(models['kwta_0_1']['alexnet'], kWTA, sr=0.1)\n",
        "models['kwta_0_1']['alexnet'].load_state_dict(torch.load(\n",
        "        DATA_DIRECTORY + 'models/kwta_0_1/AlexNet.pth'))\n",
        "\n",
        "## ResNet ReLU\n",
        "# Based off pretrained ResNet.\n",
        "# Trained for 1 epoch, test accuracy: 90.8%\n",
        "# Vanilla attack with 10 minibatches:\n",
        "#   Attacks Succeeded: 1440 / 1440 = 100.0 %\n",
        "#   Robustness Accuracy:  0 / 1440 = 0.0 %\n",
        "models['relu']['resnet'] = copy.deepcopy(models['pretrained']['resnet'])\n",
        "models['relu']['resnet'].fc.out_features = 10\n",
        "models['relu']['resnet'].load_state_dict(torch.load(\n",
        "        DATA_DIRECTORY + 'models/relu/ResNet18.pth'))\n",
        "\n",
        "## ResNet kWTA 0.2\n",
        "# Based off ReLU ResNet trained for 1 epoch.\n",
        "# Trained for 1 epoch, test accuracy: 86.3%\n",
        "# Vanilla attack with 10 minibatches:\n",
        "#   Attacks Succeeded: 721 / 1440 = 50.06944444444444 %\n",
        "#   Robustness Accuracy:  65 / 1440 = 4.513888888888889 %\n",
        "models['kwta_0_2']['resnet'] = copy.deepcopy(models['relu']['resnet'])\n",
        "activation_to_kwta(models['kwta_0_2']['resnet'], nn.ReLU, sr=0.2)\n",
        "models['kwta_0_2']['resnet'].load_state_dict(torch.load(\n",
        "        DATA_DIRECTORY + 'models/kwta_0_2/ResNet18.pth'))\n",
        "\n",
        "## ResNet kWTA 0.1\n",
        "# Based off kWTA 0.2 ResNet trained for 1 epoch.\n",
        "# Trained for 1 epoch, test accuracy: \n",
        "# Vanilla attack with 10 minibatches:\n",
        "\n",
        "models['kwta_0_1']['resnet'] = copy.deepcopy(models['kwta_0_2']['resnet'])\n",
        "activation_to_kwta(models['kwta_0_1']['resnet'], kWTA, sr=0.1)\n",
        "\n",
        "\n",
        "# TODO: kWTA seems to be slower than ReLU, training taking longer. Benchmark this."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91zwtdcmlJt6",
        "colab_type": "text"
      },
      "source": [
        "## Training Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flduWRn9lJAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Trains the model in-place, and saves after every epoch to save_path.\n",
        "# Only trains 1 epoch by default\n",
        "def train(model, save_path, epochs=1):\n",
        "    model = model.to(device) # use CUDA\n",
        "    model.train()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            inputs, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward\n",
        "            outputs = model(inputs)\n",
        "            # backward\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            # had to add clipping to fix exploding gradients:\n",
        "            nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item() / 200\n",
        "            if i % 200 == 199:    # print every 200 mini-batches\n",
        "                print('[%d, %5d] loss: %.3f' %\n",
        "                      (epoch + 1, i + 1, running_loss))\n",
        "                running_loss = 0.0\n",
        "\n",
        "        # save checkpoint after every epoch\n",
        "        torch.save(model.state_dict(), save_path)\n",
        "\n",
        "    print('Finished Training')\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rv0AsX0NmxDy",
        "colab_type": "code",
        "outputId": "ba63b878-10f7-4e0e-c86c-3b9a6a9fc325",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "#################################################\n",
        "# WARNING:                                      #\n",
        "# This will overwrite existing saved model!     #\n",
        "#################################################\n",
        "train(models['kwta_0_1']['resnet'], save_path=DATA_DIRECTORY+'models/kwta_0_1/ResNet18.pth')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1,   200] loss: 1.089\n",
            "[1,   400] loss: 0.947\n",
            "[1,   600] loss: 0.953\n",
            "[1,   800] loss: 0.909\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Xkixwy9rmzY",
        "colab_type": "text"
      },
      "source": [
        "## Test Model Accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0L-pmnCtnh0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test the model\n",
        "def test(net):\n",
        "    net = net.to(device)\n",
        "    net.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[0].to(device), data[1].to(device)\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the 10000 test images: %.1f %%' % (\n",
        "        100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrOD0Z5RpKcd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test(models['kwta_0_1']['resnet'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x83TVSQNZFp",
        "colab_type": "text"
      },
      "source": [
        "## Attacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cj7JBR-eeoJa",
        "colab_type": "text"
      },
      "source": [
        "### Vanilla Gradient Ascent\n",
        "Code taken from Fooling Images problem in PS2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwRItsV94CmE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_fooling_image(X, target_y, model, max_iter=100, debug=True):\n",
        "    \"\"\"\n",
        "    Generate a fooling image that is close to X, but that the model classifies\n",
        "    as target_y.\n",
        "    Inputs:\n",
        "    - X: Input image; Tensor of shape (1, 3, 224, 224)\n",
        "    - target_y: An integer in the range [0, 1000)\n",
        "    - model: A pretrained CNN\n",
        "    Returns:\n",
        "    - X_fooling: An image that is close to X, but that is classifed as target_y\n",
        "    by the model.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize our fooling image to the input image, and wrap it in a Variable.\n",
        "    X_fooling = X.clone()\n",
        "    X_fooling_var = Variable(X_fooling, requires_grad=True)\n",
        "\n",
        "    learning_rate = 10 # fixed learning rate\n",
        "    \n",
        "    for it in range(max_iter):\n",
        "    ##############################################################################\n",
        "    # Generate a fooling image X_fooling that the model will classify as #\n",
        "    # the class target_y. You should perform gradient ascent on the score of the #\n",
        "    # target class, stopping when the model is fooled. #\n",
        "    # When computing an update step, first normalize the gradient: #\n",
        "    # dX = learning_rate * g / ||g||_2 #\n",
        "    ##############################################################################\n",
        "        scores = model(X_fooling_var) # only one image\n",
        "        target_score = scores[:, target_y]\n",
        "        if debug:\n",
        "            print(\"Iteration: %d, Target Score: %d\" % (it, target_score.data))\n",
        "        if scores.argmax() == target_y:\n",
        "            break\n",
        "        target_score.backward()\n",
        "        image_grad = X_fooling_var.grad.data\n",
        "        dX = learning_rate * image_grad / image_grad.norm()\n",
        "        X_fooling_var.data += dX # gradient *ascent*, so adding not subtracting dX\n",
        "\n",
        "    X_fooling = X_fooling_var.data\n",
        "\n",
        "    return X_fooling, it"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwqo78BNinZD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def vis_fooling_img(X_orig, y_orig, X_fooling, target_y, class_names):\n",
        "    #X_fooling_np = np.array(deprocess(X_fooling.clone()))\n",
        "    #X_np = np.array(deprocess(X_orig.clone()))\n",
        "\n",
        "    #plt.subplot(1, 4, 1)\n",
        "    imshow(X_orig)\n",
        "    #plt.title(class_names[y_orig])\n",
        "    #plt.axis('off')\n",
        "\n",
        "    #plt.subplot(1, 4, 2)\n",
        "    imshow(X_fooling)\n",
        "    #plt.title(class_names[target_y])\n",
        "    #plt.axis('off')\n",
        "\n",
        "    #plt.subplot(1, 4, 3)\n",
        "    #diff = np.array(deprocess(X_fooling - X_orig, should_rescale=False))\n",
        "    imshow(X_fooling - X_orig)\n",
        "    #plt.title('Difference')\n",
        "    #plt.axis('off')\n",
        "\n",
        "    #plt.subplot(1, 4, 4)\n",
        "    #diff = np.array(deprocess(10 * (X_fooling - X_orig), should_rescale=False))\n",
        "    imshow(10*(X_fooling - X_orig))\n",
        "    #plt.title('Magnified difference (10x)')\n",
        "    #plt.axis('off')\n",
        "\n",
        "    #plt.gcf().set_size_inches(12, 5)\n",
        "    #plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHmi8xkYP0kj",
        "colab_type": "text"
      },
      "source": [
        "### Foolbox"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsS8j-94P11_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import foolbox\n",
        "\n",
        "# model = models['relu']['alexnet']\n",
        "# fmodel = foolbox.PyTorchModel(model)\n",
        "\n",
        "# attack = foolbox.attacks.LinfPGD()\n",
        "# epsilons = [0.0, 0.001, 0.01, 0.03, 0.1, 0.3, 0.5, 1.0]\n",
        "# _, advs, success = attack(fmodel, images, labels, epsilons=epsilons)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpVOO0etM5z5",
        "colab_type": "text"
      },
      "source": [
        "### Run attacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFZdHrR_NOVQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def attack(model_in, debug=False, num_minibatches=1):\n",
        "    # copying model so we can modify it\n",
        "    import copy\n",
        "    model = copy.deepcopy(model_in)\n",
        "    # transfer to GPU for CUDA\n",
        "    model = model.to(device)\n",
        "    # put into evaluation mode\n",
        "    model.eval()\n",
        "    # Not going to train the model, so don't compute gradients w.r.t. parameters.\n",
        "    # Using this instead of `with torch.no_grad()` because we still want gradients w.r.t. inputs.\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "    \n",
        "    if debug:\n",
        "        print(model)\n",
        "\n",
        "    # dataiter = iter(testloader)\n",
        "    # images, labels = dataiter.next()\n",
        "    # images = images.to(device)\n",
        "    # labels = labels.to(device)\n",
        "\n",
        "        # for data in testloader:\n",
        "        #     images, labels = data[0].to(device), data[1].to(device)\n",
        "        #     outputs = net(images)\n",
        "        #     _, predicted = torch.max(outputs.data, 1)\n",
        "        #     total += labels.size(0)\n",
        "        #     correct += (predicted == labels).sum().item()\n",
        "\n",
        "    # Adversarial attack loop:\n",
        "    num_robust = 0 # count how many times model's prediction was still correct after attack\n",
        "    num_fooled = 0 # count how many time the attack succeeded with the target class\n",
        "    total = 0\n",
        "    for j, data in enumerate(trainloader, 0):\n",
        "        if j >= num_minibatches:\n",
        "            break\n",
        "\n",
        "        images, labels = data[0].to(device), data[1].to(device)\n",
        "\n",
        "        # TODO: Try to parallelize batch processing instead of one at a time\n",
        "\n",
        "        for i in range(len(images)): # for each input image to attack\n",
        "            for target_idx in range(10): # for each target label\n",
        "                # skip if attack target is already correct label\n",
        "                if target_idx == labels[i]:\n",
        "                    continue\n",
        "                # attempt to make a fooling image with vanilla gradient ascent attack\n",
        "                X_fooling, num_iter = make_fooling_image(images[i].unsqueeze(0), target_idx, model, max_iter=20, debug=debug)\n",
        "                # evaluate fooling image with the model\n",
        "                scores = model(X_fooling)\n",
        "                is_fooled = scores.data.max(1)[1][0] == target_idx\n",
        "                is_robust = scores.data.max(1)[1][0] == labels[i]\n",
        "                if debug:\n",
        "                    if is_fooled:\n",
        "                        print('Fooled model, iterations =', num_iter)\n",
        "                    else:\n",
        "                        print('Failed to fool model!')\n",
        "                    X_fooling = X_fooling.cpu()\n",
        "                    # Visualize fooling image and original image differences\n",
        "                    #vis_fooling_img(images[i].cpu(), labels[i], X_fooling.cpu().squeeze(), target_idx, classes)\n",
        "                num_fooled += is_fooled\n",
        "                num_robust += is_robust\n",
        "                total += 1\n",
        "        # Print stats after every minibatch\n",
        "        print('[Minibatch: %d] Fooled: %d, Robust: %d, Total: %d' % (j+1, num_fooled, num_robust, total))\n",
        "    \n",
        "    print(\"Attacks Succeeded:\", num_fooled.item(), \"/\", total, \"=\", 100*num_fooled.item()/total, \"%\")\n",
        "    print(\"Robustness Accuracy: \", num_robust.item(), \"/\", total, \"=\", 100*num_robust.item()/total, \"%\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHam6BQpPwVL",
        "colab_type": "code",
        "outputId": "4735938f-31a2-49c3-a822-442fe35af97b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "attack(models['kwta_0_2']['resnet'], num_minibatches=10, debug=False)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Minibatch: 1] Fooled: 74, Robust: 4, Total: 144\n",
            "[Minibatch: 2] Fooled: 153, Robust: 11, Total: 288\n",
            "[Minibatch: 3] Fooled: 226, Robust: 21, Total: 432\n",
            "[Minibatch: 4] Fooled: 297, Robust: 25, Total: 576\n",
            "[Minibatch: 5] Fooled: 363, Robust: 34, Total: 720\n",
            "[Minibatch: 6] Fooled: 437, Robust: 39, Total: 864\n",
            "[Minibatch: 7] Fooled: 507, Robust: 43, Total: 1008\n",
            "[Minibatch: 8] Fooled: 580, Robust: 46, Total: 1152\n",
            "[Minibatch: 9] Fooled: 651, Robust: 49, Total: 1296\n",
            "[Minibatch: 10] Fooled: 721, Robust: 65, Total: 1440\n",
            "Attacks Succeeded: 721 / 1440 = 50.06944444444444 %\n",
            "Robustness Accuracy:  65 / 1440 = 4.513888888888889 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvQ-7CoA6PTY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}